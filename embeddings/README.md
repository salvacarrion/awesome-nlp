# Embeddings

## Word Embedding

### Word2Vec

[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781), Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean

1. Continuous Bag-of-Words (CBOW)
2. Skip-gram

### GloVe

[GloVe: Global Vectors for Word Representation](https://www-nlp.stanford.edu/pubs/glove.pdf), Jeffrey Pennington, Richard Socher, Christopher D. Manning

### FastText

[Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf), Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov

## Sentence Embedding

### ELMo

[Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf), Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer

### InferSent

[Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/abs/1705.02364), Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes

### Sentence-BERT

[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf), Nils Reimers and Iryna Gurevych